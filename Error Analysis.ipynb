{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import Some Tensorflow and Keras related packages & some model APIs"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom keras.applications.xception import preprocess_input\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\nThis version uses an Xception Net as the base model. The final layer is removed and replaced with 2 Dense (FC) layers, with a dropout layer (65% and 25%) before each of them respectively. The base layers are frozen and the 2 FC layers are trained for 8 epochs till convergence. Then, the base layers are unfrozen and the entire model is trained with a very low learning rate (1e-05) for 20 epochs. \n\nFor this error analysis, we load the model we had trained earlier, for a total of 30 epochs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.load_model('../input/pretrained-model-5/model_5.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input Pipeline\n\nIn the following section, an input pipleline is generated. We are only concerned about the validation data, as we will perform our error analysis. Test data doesn't have label, and train data has already been learnt by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing validation labels from val_annotations.txt\nval_data = pd.read_csv('/kaggle/input/image-detect/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\nval_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\nval_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing the 3 separated data generators for train/validation/test\n# Data augmentation applied only on train data\n\nvalid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model / data parameters\nnum_classes = 200\nn_x = 150 # input width\nn_y = 150 # input height\nn_c = 3 # number of channels ('rgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the generators\n\n#validation generator : labels from val_data, created earlier\nvalidation_generator = valid_datagen.flow_from_dataframe(val_data, \n                                                         directory='/kaggle/input/image-detect/val/images/', \n                                                         x_col='File', y_col='Class', \n                                                         target_size=(n_x, n_y),\n                                                         color_mode='rgb', \n                                                         class_mode='categorical', \n                                                         batch_size=256, \n                                                         shuffle=False, \n                                                         seed=42)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a dictionary mapping classes to corresponding word descriptions (wnids.txt to words.txt)\nwords = pd.read_csv('/kaggle/input/image-detect/words.txt', sep='\\t', header=None, names=['Class', 'Words'])\nword_id = pd.read_csv('/kaggle/input/image-detect/wnids.txt', sep='\\t', header=None, names=['Class'])['Class'].values\nid2words = {}\nfor ids in word_id:\n    id2words[ids] = words.loc[words['Class']==ids,'Words'].to_string(index=False).split(\",\")[0]\n    \n#print(id2words)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Generator Images\n\nThis section creates a grid to visualize the inputs after applied augemntation "},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import cm\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport math\n%matplotlib inline\n\ndef show_grid(image_list,nrows,ncols,label_list=None,show_labels=False,savename=None,figsize=(10,10),showaxis='off'):\n    if type(image_list) is not list:\n        if(image_list.shape[-1]==1):\n            image_list = [image_list[i,:,:,0] for i in range(image_list.shape[0])]\n        elif(image_list.shape[-1]==3):\n            image_list = [image_list[i,:,:,:] for i in range(image_list.shape[0])]\n    fig = plt.figure(None, figsize,frameon=False)\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(nrows, ncols),  # creates 2x2 grid of axes\n                     axes_pad=0.3,  # pad between axes in inch.\n                     share_all=True,\n                     )\n    for i in range(nrows*ncols):\n        ax = grid[i]\n        ax.imshow(image_list[i],cmap='Greys_r')  # The AxesGrid object work as a list of axes.\n        ax.axis('off')\n        if show_labels:\n            ax.set_title(id2words[class_mapping[y_int[i]]])\n    if savename != None:\n        plt.savefig(savename,bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_mapping = {v:k for k,v in validation_generator.class_indices.items()}\n\nx,y = next(validation_generator)\n#Get class int val from one hot encoded labels\ny_int = np.argmax(y,axis=-1)\n\n# Visualising the validation dataset\nshow_grid(x,4,8,label_list=y,show_labels=True,figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validation generator : labels from val_data, created earlier\nvalidation_generator = valid_datagen.flow_from_dataframe(val_data, \n                                                         directory='/kaggle/input/image-detect/val/images/', \n                                                         x_col='File', y_col='Class', \n                                                         target_size=(n_x, n_y),\n                                                         color_mode='rgb', \n                                                         class_mode='categorical', \n                                                         batch_size=1000, \n                                                         shuffle=True, \n                                                         seed=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Error Analysis\n\nWe get predictions and perform 2 analyses:\n\n* Randomly visualizing some of the incorrect labels\n* Frequency of incorrect predictions for every class\n\nSince our validation generator is randomly shuffling the inputs, any sequential subset of the inputs that we consider should give us a goos approximate of the general performance of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = next(validation_generator)\ny_int = np.argmax(y,axis=-1)\nlabels = []\nfor i in y_int:\n    labels.append(class_mapping[i])\n    \nprint(labels[:5])\nprint(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=model.predict(x)\npredicted_class_indices=np.argmax(pred,axis=1)\nlabels = (validation_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predictions[:5])\nprint(len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = []\nfor i in y_int:\n    labels.append(class_mapping[i])\n    \nprint(labels[:5])\nprint(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_preds = []\nfor i in range(len(predictions)):\n    if predictions[i]!= labels[i]:\n        wrong_preds.append(i)\n    \nprint(len(wrong_preds))\nprint(len(wrong_preds)/len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_images = []\npredicted_labels = []\nactual_labels = []\npredicted_classes = []\nactual_classes = []\n\nfor i in wrong_preds:\n    wrong_images.append(x[i])\n    predicted_labels.append(predictions[i])\n    actual_labels.append(labels[i])\n    predicted_classes.append(id2words[predictions[i]])\n    actual_classes.append(id2words[labels[i]])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 51\nplt.imshow(wrong_images[i])\nprint(f\"Predicted Label: {id2words[predicted_labels[i]]}\")\nprint(f\"Correct Label: {id2words[actual_labels[i]]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#computes frequency of incorrect predictions for every class\n\n(unique, counts) = np.unique(actual_classes, return_counts=True)\nfrequencies = np.asarray((unique, counts)).T\n\nprint(frequencies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}